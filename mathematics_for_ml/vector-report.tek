\documentclass{article}

% Preamble: Include necessary packages
\usepackage{amsmath} % For advanced math environments like vmatrix
\usepackage{amssymb} % For various math symbols
\usepackage{geometry} % To set margins
\usepackage{hyperref} % For hyperlinks if needed in the future

% Set page geometry
\geometry{a4paper, margin=1in}

% Document Title Information
\title{Vector Operations: Geometric and Algebraic Perspectives with Machine Learning Insights}
\date{}

\begin{document}

\maketitle

\section{Introduction}

Vectors are mathematical objects that possess both \textbf{magnitude} (length) and \textbf{direction}. They are distinct from scalars, which only have magnitude. This guide will explore the essential operations performed on vectors from both an algebraic and a geometric standpoint, using two simple 2D vectors as our running example:

Let's define our vectors:
\begin{itemize}
    \item $\vec{a} = (2, 3)$
    \item $\vec{b} = (4, 1)$
\end{itemize}

\vspace{0.75em}
\noindent\rule{\textwidth}{0.4pt}
\vspace{0.25em}

\section{Vector Addition ($\vec{a} + \vec{b}$)}

\subsection*{Algebraic Approach}
Vector addition is performed by adding the corresponding components of the vectors.
\[
\vec{a} + \vec{b} = (a_x + b_x, a_y + b_y)
\]
For our example vectors:
\[
\vec{a} + \vec{b} = (2 + 4, 3 + 1) = (6, 4)
\]

\subsection*{Geometric Meaning}
Geometrically, addition follows the \textbf{"head-to-tail" rule}. If you place the tail of $\vec{b}$ at the head of $\vec{a}$, the resulting vector, or the \textbf{resultant}, is the vector drawn from the origin (the tail of $\vec{a}$) to the head of $\vec{b}$. This resultant vector forms the diagonal of a parallelogram constructed from $\vec{a}$ and $\vec{b}$.

\subsection*{Machine Learning Context}
In ML, vector addition often appears when combining information from different sources or layers. For example:
\begin{itemize}
    \item In \textbf{neural networks}, inputs to a neuron are often aggregated as a sum of weighted vectors before applying an activation function.
    \item In \textbf{word embeddings} (e.g., Word2Vec), adding vectors can capture combined semantic meaning (e.g., \emph{king} - \emph{man} + \emph{woman} $\approx$ \emph{queen}).
\end{itemize}
Intuitively, addition in ML often represents the combination of independent influences or features.

\vspace{0.75em}
\noindent\rule{\textwidth}{0.4pt}
\vspace{0.25em}

\section{Vector Subtraction ($\vec{a} - \vec{b}$)}

\subsection*{Algebraic Approach}
Vector subtraction is performed by subtracting the corresponding components. This is equivalent to adding the negative of the second vector.
\[
\vec{a} - \vec{b} = (a_x - b_x, a_y - b_y)
\]
For our example vectors:
\[
\vec{a} - \vec{b} = (2 - 4, 3 - 1) = (-2, 2)
\]

\subsection*{Geometric Meaning}
Geometrically, the vector $\vec{a} - \vec{b}$ is the vector that connects the head of $\vec{b}$ to the head of $\vec{a}$. It visually represents the question, "What vector do I need to add to $\vec{b}$ to get to $\vec{a}$?".

\subsection*{Machine Learning Context}
In ML, subtraction often represents a \textbf{difference or error signal}:
\begin{itemize}
    \item In \textbf{gradient descent}, the parameter update involves subtracting the gradient vector (the direction of steepest ascent) from the current parameters to move towards a minimum.
    \item In \textbf{loss calculation}, the difference between predicted and actual vectors is key to quantifying model performance.
\end{itemize}
Intuitively, subtraction measures how far we are from a target or how to adjust our direction during optimization.

\vspace{0.75em}
\noindent\rule{\textwidth}{0.4pt}
\vspace{0.25em}

\section{Scalar Multiplication ($c \cdot \vec{a}$)}

\subsection*{Algebraic Approach}
Multiplying a vector by a scalar (a single number, $c$) involves multiplying each component of the vector by that scalar.
\[
c\vec{a} = (c \cdot a_x, c \cdot a_y)
\]
For example, let's multiply $\vec{a}$ by $c = 3$:
\[
3\vec{a} = (3 \cdot 2, 3 \cdot 3) = (6, 9)
\]

\subsection*{Geometric Meaning}
Scalar multiplication scales the magnitude (length) of the vector.
\begin{itemize}
    \item If $c > 1$, the vector is stretched.
    \item If $0 < c < 1$, the vector is shrunk.
    \item If $c < 0$, the vector's direction is reversed (it points the opposite way) in addition to being scaled.
\end{itemize}

\subsection*{Machine Learning Context}
Scalar multiplication commonly appears in:
\begin{itemize}
    \item \textbf{Learning rate adjustment} in gradient descent, where the gradient vector is scaled by a scalar learning rate to control the step size.
    \item \textbf{Normalization and regularization} where vectors are rescaled to control their length (e.g., weight decay).
\end{itemize}
Intuitively, scaling vectors lets us tune how strongly we move in a given direction during optimization or how much influence a feature has.

\vspace{0.75em}
\noindent\rule{\textwidth}{0.4pt}
\vspace{0.25em}

\section{Vector Multiplication}

Vector multiplication is not as straightforward as scalar multiplication and comes in two primary forms: the \textbf{Dot Product} and the \textbf{Cross Product}.

\subsection{Dot Product ($\vec{a} \cdot \vec{b}$)}

The dot product takes two vectors and returns a single \textbf{scalar} number.

\subsubsection*{Algebraic Approach}
The dot product is the sum of the products of the corresponding components.
\[
\vec{a} \cdot \vec{b} = (a_x \cdot b_x) + (a_y \cdot b_y)
\]
For our example vectors:
\[
\vec{a} \cdot \vec{b} = (2 \cdot 4) + (3 \cdot 1) = 8 + 3 = 11
\]

\subsubsection*{Intuitive Geometric Meaning}
The dot product measures how much one vector "points along" the other. It is the product of the magnitudes of the two vectors and the cosine of the angle ($\theta$) between them: $\vec{a} \cdot \vec{b} = |\vec{a}| |\vec{b}| \cos(\theta)$.
\begin{itemize}
    \item If the vectors are \textbf{perpendicular}, the angle is 90$^{\circ}$, $\cos(90^{\circ}) = 0$, so their dot product is \textbf{0}. They have no alignment.
    \item If the vectors point in a \textbf{similar direction}, the angle is acute, $\cos(\theta)$ is positive, so their dot product is \textbf{positive}.
    \item If the vectors point in \textbf{opposite directions}, the angle is obtuse, $\cos(\theta)$ is negative, so their dot product is \textbf{negative}.
\end{itemize}

\subsubsection*{Machine Learning Context}
Dot products are at the heart of many ML operations:
\begin{itemize}
    \item \textbf{Linear models and neural networks} use dot products to compute weighted sums of inputs before applying nonlinearities.
    \item \textbf{Cosine similarity} (a normalized dot product) is widely used in measuring similarity between high-dimensional feature embeddings (e.g., comparing document or image vectors).
\end{itemize}
Intuitively, the dot product measures \textbf{alignment}—how much one vector “projects” onto another, which is essential for similarity and weighted summation.

\subsection{Cross Product ($\vec{a} \times \vec{b}$)}

The cross product is an operation on two vectors in 3D space that results in a new \textbf{vector}.

\subsubsection*{Algebraic Approach (in 3D)}
For two 3D vectors $\vec{a} = (a_x, a_y, a_z)$ and $\vec{b} = (b_x, b_y, b_z)$, the cross product is calculated using a determinant:
\[
\vec{a} \times \vec{b} =
\begin{vmatrix}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\
a_x & a_y & a_z \\
b_x & b_y & b_z
\end{vmatrix}
= (a_y b_z - a_z b_y)\mathbf{i} - (a_x b_z - a_z b_x)\mathbf{j} + (a_x b_y - a_y b_x)\mathbf{k}
\]
Let's place our 2D vectors on the xy-plane in 3D: $\vec{a} = (2, 3, 0)$ and $\vec{b} = (4, 1, 0)$.
\[
\vec{a} \times \vec{b} = ((3)(0) - (0)(1))\mathbf{i} - ((2)(0) - (0)(4))\mathbf{j} + ((2)(1) - (3)(4))\mathbf{k}
\]
\[
\vec{a} \times \vec{b} = (0)\mathbf{i} - (0)\mathbf{j} + (2 - 12)\mathbf{k} = -10\mathbf{k}
\]
The resulting vector is $(0, 0, -10)$.

\subsubsection*{Intuitive Geometric Meaning}
\begin{itemize}
    \item \textbf{Direction:} The resulting vector is \textbf{perpendicular} to the plane formed by the original two vectors. The direction is determined by the \textbf{right-hand rule}.
    \item \textbf{Magnitude:} The magnitude of the resulting vector, $|\vec{a} \times \vec{b}|$, is equal to the \textbf{area of the parallelogram} formed by the original two vectors. In our example, the magnitude is $|-10| = 10$, which represents the area of the parallelogram formed by $\vec{a}$ and $\vec{b}$ on the xy-plane.
\end{itemize}

\subsubsection*{Machine Learning Context}
The cross product is less common in basic ML but appears in specialized tasks:
\begin{itemize}
    \item In \textbf{3D computer vision and graphics}, cross products are used to compute surface normals, which can be inputs to neural networks for rendering or scene understanding.
    \item In \textbf{robotics and reinforcement learning}, cross products describe torque and angular momentum when learning to control physical systems.
\end{itemize}
Intuitively, cross products capture \textbf{rotational effects} and orthogonal directions, useful in tasks involving 3D geometry.

\end{document}
